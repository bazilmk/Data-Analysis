{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Involve Malaysia - Data Science/Business Intelligence Test Module Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author: Bazil Muzaffar Kotriwala\n",
    "### Timestamp: 06-Mar-18 7:09PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import calendar\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Number Summation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 8, 7, 6]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sum_array(num_array):\n",
    "    '''\n",
    "    This function finds an array output such that output[i] is equal to the sum of all the elements of nums except nums[i].\n",
    "    :param: An input array for e.g [1,2,3,4]\n",
    "    :precondition: The number array must exist\n",
    "    :return: A new array equal to the sum of all the elements of nums except nums[i].\n",
    "    :complexity: Best Case = Worst Case = O(n), where n is the size of the list\n",
    "    '''\n",
    "    \n",
    "    total_sum = 0\n",
    "    summation_array = []\n",
    "    \n",
    "    # Get total sum of input array\n",
    "    for number in num_array:\n",
    "        total_sum += number\n",
    "    \n",
    "    # Calculate new number at each index and append it into the new list\n",
    "    for except_num in num_array:\n",
    "        summation_array.append(total_sum - except_num)\n",
    "    \n",
    "    return summation_array\n",
    "\n",
    "test_array = [1,2,3,4]\n",
    "sum_array(test_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Sales Data Exploration and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product_Code</th>\n",
       "      <th>W0</th>\n",
       "      <th>W1</th>\n",
       "      <th>W2</th>\n",
       "      <th>W3</th>\n",
       "      <th>W4</th>\n",
       "      <th>W5</th>\n",
       "      <th>W6</th>\n",
       "      <th>W7</th>\n",
       "      <th>W8</th>\n",
       "      <th>...</th>\n",
       "      <th>Normalized 42</th>\n",
       "      <th>Normalized 43</th>\n",
       "      <th>Normalized 44</th>\n",
       "      <th>Normalized 45</th>\n",
       "      <th>Normalized 46</th>\n",
       "      <th>Normalized 47</th>\n",
       "      <th>Normalized 48</th>\n",
       "      <th>Normalized 49</th>\n",
       "      <th>Normalized 50</th>\n",
       "      <th>Normalized 51</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P1</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P2</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P3</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P4</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P5</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Product_Code  W0  W1  W2  W3  W4  W5  W6  W7  W8      ...        \\\n",
       "0           P1  11  12  10   8  13  12  14  21   6      ...         \n",
       "1           P2   7   6   3   2   7   1   6   3   3      ...         \n",
       "2           P3   7  11   8   9  10   8   7  13  12      ...         \n",
       "3           P4  12   8  13   5   9   6   9  13  13      ...         \n",
       "4           P5   8   5  13  11   6   7   9  14   9      ...         \n",
       "\n",
       "   Normalized 42  Normalized 43  Normalized 44  Normalized 45  Normalized 46  \\\n",
       "0           0.06           0.22           0.28           0.39           0.50   \n",
       "1           0.20           0.40           0.50           0.10           0.10   \n",
       "2           0.27           1.00           0.18           0.18           0.36   \n",
       "3           0.41           0.47           0.06           0.12           0.24   \n",
       "4           0.27           0.53           0.27           0.60           0.20   \n",
       "\n",
       "   Normalized 47  Normalized 48  Normalized 49  Normalized 50  Normalized 51  \n",
       "0           0.00           0.22           0.17           0.11           0.39  \n",
       "1           0.40           0.50           0.10           0.60           0.00  \n",
       "2           0.45           1.00           0.45           0.45           0.36  \n",
       "3           0.35           0.71           0.35           0.29           0.35  \n",
       "4           0.20           0.13           0.53           0.33           0.40  \n",
       "\n",
       "[5 rows x 107 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part a: Read Sales data-set\n",
    "weekly_sales_data = pd.read_csv('Sales_Transactions_Dataset_Weekly (1).csv')\n",
    "weekly_sales_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b: Best Performing Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best performing product is: P409\n"
     ]
    }
   ],
   "source": [
    "# The best performing product is the product with the highest number of sales over the 52 weeks\n",
    "\n",
    "# Read Sales data-set\n",
    "weekly_sales_data = pd.read_csv('Sales_Transactions_Dataset_Weekly (1).csv')\n",
    "\n",
    "# Finding total sold for each product across 52 weeks\n",
    "weekly_sales_data['total_sold'] = weekly_sales_data.iloc[:,1:53].sum(axis=1)\n",
    "\n",
    "# Finding best product with the highest volume of sales\n",
    "best_product = weekly_sales_data.Product_Code[weekly_sales_data['total_sold'].idxmax()]\n",
    "print('The best performing product is:', best_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part c: Most Promising Product (Emerging Product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most promising product is P674 with an average rise in sales by 3.038%\n"
     ]
    }
   ],
   "source": [
    "# To find the most promising product, we find avg sale in first 26 weeks and remaining 26 weeks for each product respectively\n",
    "# The one with the highest rise in average sales will be the emerging product\n",
    "\n",
    "# Read data-set\n",
    "weekly_sales_data = pd.read_csv('Sales_Transactions_Dataset_Weekly (1).csv')\n",
    "\n",
    "# Create 3 new columns\n",
    "weekly_sales_data['first_26_weeks_avg'] = weekly_sales_data.iloc[:, 1:27].mean(axis=1)\n",
    "weekly_sales_data['remaining_26_weeks_avg'] = weekly_sales_data.iloc[:, 27:53].mean(axis=1)\n",
    "weekly_sales_data['change_in_avg'] = weekly_sales_data['remaining_26_weeks_avg'] - weekly_sales_data['first_26_weeks_avg']\n",
    "\n",
    "# Find product with the max increase in average sales\n",
    "product_max_rise = weekly_sales_data.Product_Code[weekly_sales_data['change_in_avg'].idxmax()]\n",
    "max_rise_value = weekly_sales_data['change_in_avg'].max()\n",
    "print('The most promising product is ' + product_max_rise + ' with an average rise in sales by ' + str(round(max_rise_value, 3)) +'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part d: Worst Performing Product (biweekly basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The worst performing product based on lowest average biweekly sales is: P215\n"
     ]
    }
   ],
   "source": [
    "# To calculate worst performing product, we will first find total product amount sold on a biweekly basis instead of a weekly basis\n",
    "# Then find the product with the average across all the biweeks for all products\n",
    "# One with the lowest average will be the worst performing product\n",
    "\n",
    "# Read data-set\n",
    "weekly_sales_data = pd.read_csv('Sales_Transactions_Dataset_Weekly (1).csv')\n",
    "weekly_sales_pcode = weekly_sales_data['Product_Code']\n",
    "\n",
    "# Slice df to contain only week 0 - 51\n",
    "weekly_sales_data = weekly_sales_data.iloc[:, 1:53]\n",
    "\n",
    "# Create biweekly sales columns for each product\n",
    "weekly_sales_data = pd.DataFrame(np.add.reduceat(weekly_sales_data.values, np.arange(len(weekly_sales_data.columns))[::2], axis=1))\n",
    "weekly_sales_data['Product_Code'] = weekly_sales_pcode\n",
    "weekly_sales_data.set_index('Product_Code', inplace=True)\n",
    "\n",
    "# Finding the average of biweekly sales of each product\n",
    "mean_products = weekly_sales_data.iloc[:].mean(axis=1)\n",
    "mean_products = pd.DataFrame(mean_products)\n",
    "\n",
    "# Finding worst product\n",
    "worst_product = mean_products[0].idxmin()\n",
    "print('The worst performing product based on lowest average biweekly sales is:', worst_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part e: Outliers in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     weekly_mean  weekly_median  weekly_std  weekly_sum\n",
      "W15    10.045623            4.0   13.743655        8147\n",
      "W16    10.033292            4.0   13.890316        8137\n",
      "W24    10.166461            5.0   12.298777        8245\n",
      "\n",
      "The following weeks are outlier weeks: W15 W16 W24\n"
     ]
    }
   ],
   "source": [
    "# Read data-set\n",
    "weekly_sales_data = pd.read_csv('Sales_Transactions_Dataset_Weekly (1).csv')\n",
    "\n",
    "# Create new dataframe containing mean, median, sd and sum of each week\n",
    "weekly_descriptive_df = pd.DataFrame()\n",
    "weekly_descriptive_df['weekly_mean'] = weekly_sales_data.iloc[:, 1:53].mean(axis=0)\n",
    "weekly_descriptive_df['weekly_median'] = weekly_sales_data.iloc[:, 1:53].median(axis=0)\n",
    "weekly_descriptive_df['weekly_std'] = weekly_sales_data.iloc[:, 1:53].std(axis=0)\n",
    "weekly_descriptive_df['weekly_sum'] = weekly_sales_data.iloc[:, 1:53].sum(axis=0)\n",
    "\n",
    "# Identify the weeks \n",
    "outlier_weeks = weekly_descriptive_df[weekly_descriptive_df.weekly_mean > np.percentile(weekly_descriptive_df.weekly_mean,95)]\n",
    "print(outlier_weeks)\n",
    "outlier_weeks = outlier_weeks.index.tolist()\n",
    "print()\n",
    "print(*['The following weeks are outlier weeks:'] + outlier_weeks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Job Posts Data Exploration and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobpost</th>\n",
       "      <th>date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>AnnouncementCode</th>\n",
       "      <th>Term</th>\n",
       "      <th>Eligibility</th>\n",
       "      <th>Audience</th>\n",
       "      <th>StartDate</th>\n",
       "      <th>Duration</th>\n",
       "      <th>...</th>\n",
       "      <th>Salary</th>\n",
       "      <th>ApplicationP</th>\n",
       "      <th>OpeningDate</th>\n",
       "      <th>Deadline</th>\n",
       "      <th>Notes</th>\n",
       "      <th>AboutC</th>\n",
       "      <th>Attach</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>IT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMERIA Investment Consulting Company\\r\\nJOB TI...</td>\n",
       "      <td>Jan 5, 2004</td>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>AMERIA Investment Consulting Company</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>To apply for this position, please submit a\\r\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26 January 2004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2004</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>International Research &amp; Exchanges Board (IREX...</td>\n",
       "      <td>Jan 7, 2004</td>\n",
       "      <td>Full-time Community Connections Intern (paid i...</td>\n",
       "      <td>International Research &amp; Exchanges Board (IREX)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3 months</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Please submit a cover letter and resume to:\\r\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12 January 2004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The International Research &amp; Exchanges Board (...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2004</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Caucasus Environmental NGO Network (CENN)\\r\\nJ...</td>\n",
       "      <td>Jan 7, 2004</td>\n",
       "      <td>Country Coordinator</td>\n",
       "      <td>Caucasus Environmental NGO Network (CENN)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Renewable annual contract\\r\\nPOSITION</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Please send resume or CV toursula.kazarian@......</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20 January 2004\\r\\nSTART DATE:  February 2004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Caucasus Environmental NGO Network is a\\r\\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2004</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Manoff Group\\r\\nJOB TITLE:  BCC Specialist\\r\\n...</td>\n",
       "      <td>Jan 7, 2004</td>\n",
       "      <td>BCC Specialist</td>\n",
       "      <td>Manoff Group</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Please send cover letter and resume to Amy\\r\\n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23 January 2004\\r\\nSTART DATE:  Immediate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2004</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yerevan Brandy Company\\r\\nJOB TITLE:  Software...</td>\n",
       "      <td>Jan 10, 2004</td>\n",
       "      <td>Software Developer</td>\n",
       "      <td>Yerevan Brandy Company</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Successful candidates should submit\\r\\n- CV; \\...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20 January 2004, 18:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2004</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             jobpost          date  \\\n",
       "0  AMERIA Investment Consulting Company\\r\\nJOB TI...   Jan 5, 2004   \n",
       "1  International Research & Exchanges Board (IREX...   Jan 7, 2004   \n",
       "2  Caucasus Environmental NGO Network (CENN)\\r\\nJ...   Jan 7, 2004   \n",
       "3  Manoff Group\\r\\nJOB TITLE:  BCC Specialist\\r\\n...   Jan 7, 2004   \n",
       "4  Yerevan Brandy Company\\r\\nJOB TITLE:  Software...  Jan 10, 2004   \n",
       "\n",
       "                                               Title  \\\n",
       "0                            Chief Financial Officer   \n",
       "1  Full-time Community Connections Intern (paid i...   \n",
       "2                                Country Coordinator   \n",
       "3                                     BCC Specialist   \n",
       "4                                 Software Developer   \n",
       "\n",
       "                                           Company AnnouncementCode Term  \\\n",
       "0             AMERIA Investment Consulting Company              NaN  NaN   \n",
       "1  International Research & Exchanges Board (IREX)              NaN  NaN   \n",
       "2        Caucasus Environmental NGO Network (CENN)              NaN  NaN   \n",
       "3                                     Manoff Group              NaN  NaN   \n",
       "4                           Yerevan Brandy Company              NaN  NaN   \n",
       "\n",
       "  Eligibility Audience StartDate                               Duration  \\\n",
       "0         NaN      NaN       NaN                                    NaN   \n",
       "1         NaN      NaN       NaN                               3 months   \n",
       "2         NaN      NaN       NaN  Renewable annual contract\\r\\nPOSITION   \n",
       "3         NaN      NaN       NaN                                    NaN   \n",
       "4         NaN      NaN       NaN                                    NaN   \n",
       "\n",
       "   ...   Salary                                       ApplicationP  \\\n",
       "0  ...      NaN  To apply for this position, please submit a\\r\\...   \n",
       "1  ...      NaN  Please submit a cover letter and resume to:\\r\\...   \n",
       "2  ...      NaN  Please send resume or CV toursula.kazarian@......   \n",
       "3  ...      NaN  Please send cover letter and resume to Amy\\r\\n...   \n",
       "4  ...      NaN  Successful candidates should submit\\r\\n- CV; \\...   \n",
       "\n",
       "  OpeningDate                                       Deadline Notes  \\\n",
       "0         NaN                                26 January 2004   NaN   \n",
       "1         NaN                                12 January 2004   NaN   \n",
       "2         NaN  20 January 2004\\r\\nSTART DATE:  February 2004   NaN   \n",
       "3         NaN      23 January 2004\\r\\nSTART DATE:  Immediate   NaN   \n",
       "4         NaN                         20 January 2004, 18:00   NaN   \n",
       "\n",
       "                                              AboutC Attach  Year Month     IT  \n",
       "0                                                NaN    NaN  2004     1  False  \n",
       "1  The International Research & Exchanges Board (...    NaN  2004     1  False  \n",
       "2  The Caucasus Environmental NGO Network is a\\r\\...    NaN  2004     1  False  \n",
       "3                                                NaN    NaN  2004     1  False  \n",
       "4                                                NaN    NaN  2004     1   True  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part a: Read Job Post data-set\n",
    "job_posts_df = pd.read_csv('data job posts.csv')\n",
    "job_posts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b: Extract fields from jobpost column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read data-set\n",
    "job_posts_df = pd.read_csv('data job posts.csv')\n",
    "\n",
    "# Create lists for each respective job post and remove \\r\\n\n",
    "def process_jobposts():\n",
    "    return job_posts_df['jobpost'].apply(lambda x: x.split('\\r\\n'))\n",
    "\n",
    "# Store the respective positions in a dictionary\n",
    "def create_field_df():\n",
    "    field_dict = {'JOB TITLE': [], 'POSITION LOCATION': [], 'POSITION DURATION': [], 'JOB DESCRIPTION': [],\n",
    "                  'JOB RESPONSIBILITIES': [],'REQUIRED QUALIFICATIONS': [], 'REMUNERATION': [], 'APPLICATION DEADLINE': [], \n",
    "                  'ABOUT COMPANY': []}\n",
    "    \n",
    "    for i in range(len(job_post_lists)):\n",
    "        for j in range(len(job_post_lists[i])):\n",
    "            if len(job_post_lists[i]) > 1 and job_post_lists[i][j] != '' and 'TITLE' in job_post_lists[i][j]:\n",
    "                field_dict['JOB TITLE'].append(job_post_lists[i][j].replace('JOB TITLE:', '').replace('TITLE:', '').strip(' '))\n",
    "            if len(job_post_lists[i]) > 1 and job_post_lists[i][j] != '' and 'LOCATION' in job_post_lists[i][j]:\n",
    "                field_dict['POSITION LOCATION'].append(job_post_lists[i][j].replace('POSITION LOCATION:', '').replace('LOCATION:', '').strip(' '))\n",
    "            if len(job_post_lists[i]) > 1 and job_post_lists[i][j] != '' and 'DURATION' in job_post_lists[i][j]:\n",
    "                field_dict['POSITION DURATION'].append(job_post_lists[i][j].replace('POSITION DURATION:', '').replace('DURATION:', '').strip(' '))\n",
    "            if len(job_post_lists[i]) > 1 and job_post_lists[i][j] != '' and 'DEADLINE' in job_post_lists[i][j]:\n",
    "                field_dict['APPLICATION DEADLINE'].append(job_post_lists[i][j].replace('APPLICATION DEADLINE:', '').replace('DEADLINE:', '').strip(' '))\n",
    "            if len(job_post_lists[i]) > 1 and job_post_lists[i][j] != '' and 'REMUNERATION' in job_post_lists[i][j]:\n",
    "                field_dict['REMUNERATION'].append(job_post_lists[i][j].replace('REMUNERATION:', '').replace('REMUNERATION/ SALARY', '').strip(' '))  \n",
    "    return field_dict  \n",
    "\n",
    "job_post_lists = process_jobposts()\n",
    "field_dict = create_field_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part c: Company with most job ads in past 2 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The company with the most job ads in the past 2 years is: mentorgraphicsdevelopmentservices\n"
     ]
    }
   ],
   "source": [
    "# Fill missing values\n",
    "job_posts_df['Company'] = job_posts_df['Company'].fillna('NULL')\n",
    "\n",
    "# Convert date strings to date-time format\n",
    "job_posts_df['date'] = pd.to_datetime(job_posts_df['date'], errors = 'coerce')\n",
    "\n",
    "# Create data-frame containing all the data from the past 2 years\n",
    "end_date = job_posts_df['date'][len(job_posts_df['date']) - 1]\n",
    "start_date = yesterday = end_date - relativedelta(years=2)\n",
    "jobs_2y_df = job_posts_df.loc[(job_posts_df['date'] > start_date) & (job_posts_df['date'] <= end_date)]\n",
    "\n",
    "# Clean last two years data\n",
    "def remove_punctuation():\n",
    "    return jobs_2y_df['Company'].apply(lambda x: re.sub('[^A-Za-z0-9]+', '', x.lower()))\n",
    "\n",
    "def remove_tags():\n",
    "    return comp_df['Company'].apply(lambda x: x.replace('llc', '').replace('ltd', '')\n",
    "                                    .replace('consulting', '').replace('co', '').replace('cjsc', '').replace('TITLE:', '')\n",
    "                                    .replace('csjc', ''))\n",
    "\n",
    "# Call functions\n",
    "company_freq = remove_punctuation()\n",
    "comp_df = pd.DataFrame(company_freq)\n",
    "tags = remove_tags()\n",
    "comp_new_df = pd.DataFrame(remove_tags())\n",
    "\n",
    "# Find company with the max number of job posts\n",
    "max_company = comp_new_df.groupby(['Company']).size().idxmax()\n",
    "print('The company with the most job ads in the past 2 years is:', max_company)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part d: Month with largest number of job ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The month with the largest number of job ads is: September\n"
     ]
    }
   ],
   "source": [
    "# Convert date strings to date-time format\n",
    "job_posts_df['date'] = pd.to_datetime(job_posts_df['date'], errors = 'coerce')\n",
    "\n",
    "# Index rows by date\n",
    "job_posts_df.index = pd.to_datetime(job_posts_df.date)\n",
    "\n",
    "# Create column depicting month number for each date\n",
    "job_posts_df['month'] = pd.DatetimeIndex(job_posts_df['date']).month\n",
    "\n",
    "# Group by month number with respective job ads frequency\n",
    "month_number = int(job_posts_df.groupby(['month']).size().idxmax())\n",
    "\n",
    "# Extract month name from month number\n",
    "month_name = calendar.month_name[month_number]\n",
    "print('The month with the largest number of job ads is:', month_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: String Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>description_x</th>\n",
       "      <th>description_y</th>\n",
       "      <th>same_security</th>\n",
       "      <th>similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>semtech corp</td>\n",
       "      <td>semtech corporation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>vanguard mid cap index</td>\n",
       "      <td>vanguard midcap index - a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.893617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>spdr gold trust gold shares</td>\n",
       "      <td>spdr gold trust spdr gold shares</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.915254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>vanguard total bond index adm</td>\n",
       "      <td>vanguard total bond market index</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.819672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>oakmark international fund class i</td>\n",
       "      <td>oakmark international cl i</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_id                       description_x  \\\n",
       "0        0                        semtech corp   \n",
       "1        1              vanguard mid cap index   \n",
       "2        2         spdr gold trust gold shares   \n",
       "3        3       vanguard total bond index adm   \n",
       "4        4  oakmark international fund class i   \n",
       "\n",
       "                      description_y  same_security  similarity_score  \n",
       "0               semtech corporation            NaN          0.774194  \n",
       "1         vanguard midcap index - a            NaN          0.893617  \n",
       "2  spdr gold trust spdr gold shares            NaN          0.915254  \n",
       "3  vanguard total bond market index            NaN          0.819672  \n",
       "4        oakmark international cl i            NaN          0.866667  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read test data-set\n",
    "text_test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Find similarity score between description_x and description_y\n",
    "text_test_df['similarity_score'] = text_test_df.apply(lambda x: SequenceMatcher(None, x[1].strip(), x[2].strip()).ratio(), axis=1)\n",
    "text_test_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
